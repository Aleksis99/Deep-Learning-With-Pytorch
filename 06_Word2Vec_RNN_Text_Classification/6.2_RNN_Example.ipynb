{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7P_DsOsZe6qb","outputId":"e06d1b48-c760-43ed-e5eb-6d6d739e997d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Output difference (L2): 1.0106459313874439e-07\n","Hidden difference (L2): 6.143906006172983e-08\n","Outputs match: True\n","Final hidden states match: True\n"]}],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchsummary import summary\n","torch.manual_seed(0)\n","\n","# Configuration\n","input_size = 3\n","hidden_size = 5\n","seq_len = 4\n","batch_size = 2\n","\n","# Input with batch_first: (batch, seq_len, input_size)\n","# Input with batch_first: (seqL_len, batch, input_size)\n","\n","x = torch.randn(batch_size, seq_len, input_size)\n","# Built-in RNN with batch_first=True\n","rnn = nn.RNN(input_size, hidden_size, nonlinearity='relu', batch_first=True)\n","h0 = torch.zeros(1, batch_size, hidden_size)\n","\n","# Run built-in RNN\n","out_builtin, hn_builtin = rnn(x, h0)\n","\n","# Extract weights and biases\n","W_ih = rnn.weight_ih_l0.detach()\n","W_hh = rnn.weight_hh_l0.detach()\n","b_ih = rnn.bias_ih_l0.detach()\n","b_hh = rnn.bias_hh_l0.detach()\n","\n","def rnn_step(x_t, h_t_prev, W_ih, W_hh, b_ih, b_hh):\n","    return torch.relu(x_t @ W_ih.T + b_ih + h_t_prev @ W_hh.T + b_hh)\n","\n","def my_simple_rnn(x, h0, W_ih, W_hh, b_ih, b_hh):\n","    # x.shape: batch_size, seq_len, input_size\n","    batch_size, seq_len, _ = x.size()\n","    h_t = h0[0]  # Remove layer dim\n","    outputs = []\n","\n","    for t in range(seq_len):\n","        x_t = x[:, t, :]  # Shape: (batch, input_size)\n","        h_t = rnn_step(x_t, h_t, W_ih, W_hh, b_ih, b_hh)\n","        outputs.append(h_t.unsqueeze(1))  # Add seq dim\n","\n","    output_seq = torch.cat(outputs, dim=1)  # (batch, seq_len, hidden)\n","    return output_seq, h_t.unsqueeze(0)\n","\n","# Run manual RNN with same weights\n","out_manual, hn_manual = my_simple_rnn(x, h0, W_ih, W_hh, b_ih, b_hh)\n","\n","# Verify numerical equivalence\n","print(\"Output difference (L2):\", torch.norm(out_builtin - out_manual).item())\n","print(\"Hidden difference (L2):\", torch.norm(hn_builtin - hn_manual).item())\n","print(\"Outputs match:\", torch.allclose(out_builtin, out_manual, atol=1e-6))\n","print(\"Final hidden states match:\", torch.allclose(hn_builtin, hn_manual, atol=1e-6))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XxiA5okq9zW7","outputId":"6510249d-41d6-4e50-f737-5dd9e26c5a4d"},"outputs":[{"name":"stdout","output_type":"stream","text":["Output difference: 3.6263745073483733e-07\n","Hidden state difference: 2.9802322387695312e-08\n","Output match? True\n","Hidden match? True\n"]}],"source":["input_size = 3\n","hidden_size = 4\n","seq_len = 5\n","batch_size = 2\n","\n","# Input with batch_first: (batch, seq_len, input_size)\n","x = torch.randn(batch_size, seq_len, input_size)\n","\n","# Built-in RNN with ReLU and batch_first=True\n","rnn = nn.RNN(input_size, hidden_size, nonlinearity='relu', batch_first=True)\n","h0 = torch.zeros(1, batch_size, hidden_size)\n","\n","# Built-in RNN forward\n","out_builtin, hn_builtin = rnn(x, h0)\n","\n","# Custom recurrent cell with ReLU\n","class RecurrentCell(nn.Module):\n","    def __init__(self, input_size, hidden_size):\n","        super().__init__()\n","        self.ih = nn.Linear(input_size, hidden_size)\n","        self.hh = nn.Linear(hidden_size, hidden_size)\n","        self.activation = nn.ReLU()\n","\n","    def forward(self, x, h):\n","        out = self.activation(self.ih(x) + self.hh(h))\n","        return out, out\n","\n","# Create custom cell & copy weights from built-in RNN\n","cell = RecurrentCell(input_size, hidden_size)\n","with torch.no_grad():\n","    cell.ih.weight.copy_(rnn.weight_ih_l0)\n","    cell.ih.bias.copy_(rnn.bias_ih_l0)\n","    cell.hh.weight.copy_(rnn.weight_hh_l0)\n","    cell.hh.bias.copy_(rnn.bias_hh_l0)\n","\n","# Manual RNN forward pass (batch first)\n","h_manual = h0[0]  # shape: (batch, hidden_size)\n","outputs = []\n","\n","for t in range(seq_len):\n","    x_t = x[:, t, :]  # shape: (batch, input_size)\n","    h_manual, _ = cell(x_t, h_manual)\n","    outputs.append(h_manual.unsqueeze(1))  # keep time dimension\n","\n","out_manual = torch.cat(outputs, dim=1)      # (batch, seq_len, hidden)\n","hn_manual = h_manual.unsqueeze(0)           # (1, batch, hidden)\n","\n","# Compare with built-in\n","print(\"Output difference:\", torch.norm(out_builtin - out_manual).item())\n","print(\"Hidden state difference:\", torch.norm(hn_builtin - hn_manual).item())\n","print(\"Output match?\", torch.allclose(out_builtin, out_manual, atol=1e-6))\n","print(\"Hidden match?\", torch.allclose(hn_builtin, hn_manual, atol=1e-6))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2-PtOxd7CZnP"},"outputs":[],"source":["class Model(nn.Module):\n","    def __init__(self, input_size, hidden_size, num_layers):\n","        super().__init__()\n","\n","        self.input_size = input_size\n","        self.hidden_size = hidden_size\n","        self.num_layers = num_layers\n","\n","        layers = []\n","        if num_layers >= 1:\n","            layers.append(RecurrentCell(input_size, hidden_size))\n","        for _ in range(1, num_layers):\n","            layers.append(RecurrentCell(hidden_size, hidden_size))\n","\n","        self.recurrent_cells = nn.ModuleList(layers)\n","        self.dense = nn.Linear(hidden_size, input_size)\n","\n","    def forward(self, x):\n","        batch_size, seq_len = x.size(0), x.size(1)\n","        h = torch.zeros(batch_size, self.num_layers, self.hidden_size)\n","\n","        rnn_out = torch.zeros(batch_size, self.num_layers, self.hidden_size)\n","\n","        for t in range(seq_len):\n","            out = x[:, t, :]\n","            for i in range(len(self.recurrent_cells)):\n","                h_prev = h[:, i, :]\n","                out, h_updated = self.recurrent_cells[i](out, h_prev)\n","                h = h.clone()\n","                h[:, i, :] = h_updated\n","                rnn_out[:, i, :] = out\n","\n","        out = self.dense(rnn_out[:, -1, :])\n","\n","        return out\n","\n","model = Model(1, 5, 3)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BD9xtR1cCulI","outputId":"25aca5f8-26b8-4ea8-85d9-f83f9c9e46a5"},"outputs":[{"name":"stdout","output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Linear-1                    [-1, 5]              10\n","            Linear-2                    [-1, 5]              30\n","              ReLU-3                    [-1, 5]               0\n","     RecurrentCell-4         [[-1, 5], [-1, 5]]               0\n","            Linear-5                    [-1, 5]              30\n","            Linear-6                    [-1, 5]              30\n","              ReLU-7                    [-1, 5]               0\n","     RecurrentCell-8         [[-1, 5], [-1, 5]]               0\n","            Linear-9                    [-1, 5]              30\n","           Linear-10                    [-1, 5]              30\n","             ReLU-11                    [-1, 5]               0\n","    RecurrentCell-12         [[-1, 5], [-1, 5]]               0\n","           Linear-13                    [-1, 5]              10\n","           Linear-14                    [-1, 5]              30\n","             ReLU-15                    [-1, 5]               0\n","    RecurrentCell-16         [[-1, 5], [-1, 5]]               0\n","           Linear-17                    [-1, 5]              30\n","           Linear-18                    [-1, 5]              30\n","             ReLU-19                    [-1, 5]               0\n","    RecurrentCell-20         [[-1, 5], [-1, 5]]               0\n","           Linear-21                    [-1, 5]              30\n","           Linear-22                    [-1, 5]              30\n","             ReLU-23                    [-1, 5]               0\n","    RecurrentCell-24         [[-1, 5], [-1, 5]]               0\n","           Linear-25                    [-1, 5]              10\n","           Linear-26                    [-1, 5]              30\n","             ReLU-27                    [-1, 5]               0\n","    RecurrentCell-28         [[-1, 5], [-1, 5]]               0\n","           Linear-29                    [-1, 5]              30\n","           Linear-30                    [-1, 5]              30\n","             ReLU-31                    [-1, 5]               0\n","    RecurrentCell-32         [[-1, 5], [-1, 5]]               0\n","           Linear-33                    [-1, 5]              30\n","           Linear-34                    [-1, 5]              30\n","             ReLU-35                    [-1, 5]               0\n","    RecurrentCell-36         [[-1, 5], [-1, 5]]               0\n","           Linear-37                    [-1, 1]               6\n","================================================================\n","Total params: 486\n","Trainable params: 486\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.00\n","Forward/backward pass size (MB): 0.00\n","Params size (MB): 0.00\n","Estimated Total Size (MB): 0.00\n","----------------------------------------------------------------\n"]}],"source":["summary(model, (3, 1))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"isfzow6LDpNd"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}